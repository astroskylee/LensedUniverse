{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test_joint_hmc\n",
    "\n",
    "这个 notebook 内嵌 `hmc_scripts/run_joint_hmc.py` 的完整逻辑，不调用外部脚本。\n",
    "默认设置：4 probes (dspl/lens/sne/quasar), clean-only inference, target_accept_prob=0.99。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ.setdefault(\"HDF5_USE_FILE_LOCKING\", \"FALSE\")\n",
    "\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "workdir = Path.cwd()\n",
    "if (workdir / 'hmc_scripts').exists() is False:\n",
    "    workdir = Path('/users/tianli/LensedUniverse')\n",
    "os.chdir(workdir)\n",
    "if str(workdir) not in sys.path:\n",
    "    sys.path.insert(0, str(workdir))\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpyro\n",
    "import numpyro.distributions as dist\n",
    "from numpyro.infer import NUTS, MCMC, init_to_value\n",
    "from jax import random\n",
    "import arviz as az\n",
    "\n",
    "from slcosmo import tool\n",
    "from hmc_scripts.corner_utils import select_corner_vars, make_overlay_corner\n",
    "\n",
    "USE_X64 = os.environ.get(\"SLCOSMO_USE_X64\", \"0\").strip().lower() in {\"1\", \"true\", \"yes\", \"y\", \"on\"}\n",
    "jax.config.update(\"jax_enable_x64\", USE_X64)\n",
    "if USE_X64:\n",
    "    numpyro.enable_x64()\n",
    "if any(d.platform == \"gpu\" for d in jax.devices()):\n",
    "    numpyro.set_platform(\"gpu\")\n",
    "else:\n",
    "    numpyro.set_platform(\"cpu\")\n",
    "print(f\"[INFO] Precision mode: {'FP64' if USE_X64 else 'FP32'}\", flush=True)\n",
    "\n",
    "SEED = 42\n",
    "rng_np = np.random.default_rng(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "TEST_MODE = False\n",
    "RUN_NOISY_INFERENCE = False\n",
    "RESULT_DIR = Path(\"/mnt/lustre/tianli/LensedUniverse_result\")\n",
    "RESULT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "FIG_DIR = Path(\"result\")\n",
    "FIG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "DATA_DIR = Path(os.environ.get(\"SLCOSMO_DATA_DIR\", str(workdir / \"data\")))\n",
    "\n",
    "\n",
    "def step(message):\n",
    "    print(f\"[STEP] {message}\", flush=True)\n",
    "\n",
    "\n",
    "cosmo_true = {\"Omegam\": 0.32, \"Omegak\": 0.0, \"w0\": -1.0, \"wa\": 0.0, \"h0\": 70.0}\n",
    "cosmo_prior = {\n",
    "    \"w0_up\": 0.0,   \"w0_low\": -2.0,\n",
    "    \"wa_up\": 2.0,   \"wa_low\": -2.0,\n",
    "    \"omegak_up\": 1.0, \"omegak_low\": -1.0,\n",
    "    \"h0_up\": 80.0,  \"h0_low\": 60.0,\n",
    "    \"omegam_up\": 0.5, \"omegam_low\": 0.1,\n",
    "}\n",
    "DSPL_TARGET = 500\n",
    "PHOTO_FRAC_ZS2 = 0.60\n",
    "\n",
    "# ---------------------------\n",
    "# DSPL data (clean + noisy)\n",
    "# ---------------------------\n",
    "step(\"Build DSPL clean/noisy datasets\")\n",
    "data_dspl = np.loadtxt(DATA_DIR / \"EuclidDSPLs_1.txt\")\n",
    "data_dspl = data_dspl[(data_dspl[:, 5] < 0.95)]\n",
    "\n",
    "zl_dspl  = data_dspl[:, 0]\n",
    "zs1_dspl = data_dspl[:, 1]\n",
    "zs2_true_cat = data_dspl[:, 2]\n",
    "\n",
    "beta_err_dspl = data_dspl[:, 6]\n",
    "model_vel_dspl = data_dspl[:, 11]\n",
    "\n",
    "step(\n",
    "    f\"Catalog zs2<=zs1 count: {int(np.sum(zs2_true_cat <= zs1_dspl))}/\"\n",
    "    f\"{len(zs2_true_cat)} (no pre-filter)\"\n",
    ")\n",
    "\n",
    "N_all = len(zl_dspl)\n",
    "if N_all > DSPL_TARGET:\n",
    "    select_idx = np.sort(rng_np.choice(N_all, size=DSPL_TARGET, replace=False))\n",
    "    zl_dspl = zl_dspl[select_idx]\n",
    "    zs1_dspl = zs1_dspl[select_idx]\n",
    "    zs2_true_cat = zs2_true_cat[select_idx]\n",
    "    beta_err_dspl = beta_err_dspl[select_idx]\n",
    "    model_vel_dspl = model_vel_dspl[select_idx]\n",
    "N_dspl = len(zl_dspl)\n",
    "\n",
    "n_photo = int(round(PHOTO_FRAC_ZS2 * N_dspl))\n",
    "n_photo = max(0, min(n_photo, N_dspl))\n",
    "is_photo = np.zeros(N_dspl, dtype=bool)\n",
    "if n_photo > 0:\n",
    "    photo_idx = rng_np.choice(N_dspl, size=n_photo, replace=False)\n",
    "    is_photo[photo_idx] = True\n",
    "step(\n",
    "    f\"Use {N_dspl} DSPL systems; source2 photo-z count={int(is_photo.sum())} \"\n",
    "    f\"({float(is_photo.mean()):.2%})\"\n",
    ")\n",
    "zs2_err = np.where(is_photo, 0.1, 1e-4)\n",
    "zs2_obs = zs2_true_cat + rng_np.normal(0.0, zs2_err)\n",
    "\n",
    "eps = 1e-3\n",
    "bad = zs2_obs <= (zs1_dspl + eps)\n",
    "for _ in range(20):\n",
    "    if not np.any(bad):\n",
    "        break\n",
    "    zs2_obs[bad] = zs2_true_cat[bad] + rng_np.normal(0.0, zs2_err[bad])\n",
    "    bad = zs2_obs <= (zs1_dspl + eps)\n",
    "zs2_obs = np.maximum(zs2_obs, zs1_dspl + eps)\n",
    "\n",
    "Dl1, Ds1, Dls1 = tool.compute_distances(zl_dspl, zs1_dspl, cosmo_true)\n",
    "Dl2, Ds2, Dls2 = tool.compute_distances(zl_dspl, zs2_true_cat, cosmo_true)\n",
    "beta_geom_dspl = Dls1 * Ds2 / (Ds1 * Dls2)\n",
    "\n",
    "lambda_true_dspl = tool.truncated_normal(1.0, 0.05, 0.85, 1.15, N_dspl, random_state=rng_np)\n",
    "lambda_err_dspl = lambda_true_dspl * 0.10\n",
    "\n",
    "true_vel_dspl = model_vel_dspl * jnp.sqrt(lambda_true_dspl)\n",
    "vel_err_dspl = 0.03 * true_vel_dspl\n",
    "\n",
    "beta_true_dspl = tool.beta_antimst(beta_geom_dspl, mst=lambda_true_dspl)\n",
    "\n",
    "lambda_obs_dspl_clean = lambda_true_dspl\n",
    "beta_obs_dspl_clean = beta_true_dspl\n",
    "zs2_use_clean = zs2_true_cat\n",
    "\n",
    "lambda_obs_dspl_noisy = lambda_true_dspl + np.random.normal(0.0, lambda_err_dspl)\n",
    "beta_obs_dspl_noisy = tool.truncated_normal(beta_true_dspl, beta_err_dspl, 0.0, 1.0, random_state=rng_np)\n",
    "zs2_use_noisy = zs2_obs\n",
    "\n",
    "\n",
    "def build_dspl(lambda_obs, beta_obs, zs2_use):\n",
    "    return {\n",
    "        \"zl\": zl_dspl,\n",
    "        \"zs1\": zs1_dspl,\n",
    "        \"zs2_cat\": zs2_true_cat,\n",
    "        \"zs2_obs\": zs2_use,\n",
    "        \"zs2_err\": zs2_err,\n",
    "        \"is_photo\": is_photo.astype(np.int32),\n",
    "        \"beta_obs\": beta_obs,\n",
    "        \"beta_err\": beta_err_dspl,\n",
    "        \"v_model\": model_vel_dspl,\n",
    "        \"v_obs\": true_vel_dspl,\n",
    "        \"v_err\": vel_err_dspl,\n",
    "        \"lambda_err\": lambda_err_dspl,\n",
    "        \"lambda_obs\": lambda_obs,\n",
    "    }\n",
    "\n",
    "\n",
    "dspl_clean = build_dspl(lambda_obs_dspl_clean, beta_obs_dspl_clean, zs2_use_clean)\n",
    "dspl_noisy = build_dspl(lambda_obs_dspl_noisy, beta_obs_dspl_noisy, zs2_use_noisy)\n",
    "\n",
    "# ---------------------------\n",
    "# Lens+kin data (clean + noisy)\n",
    "# ---------------------------\n",
    "step(\"Build lens+kinematic clean/noisy datasets\")\n",
    "LUT = np.load(DATA_DIR / \"velocity_disp_table.npy\")\n",
    "N1, N2, N3, N4 = LUT.shape\n",
    "thetaE_grid = np.linspace(0.5, 3.0, N1)\n",
    "gamma_grid  = np.linspace(1.2, 2.8, N2)\n",
    "Re_grid     = np.linspace(0.15, 3.0, N3)\n",
    "beta_grid   = np.linspace(-0.5, 0.8, N4)\n",
    "jampy_interp = tool.make_4d_interpolant(thetaE_grid, gamma_grid, Re_grid, beta_grid, LUT)\n",
    "\n",
    "Euclid_GG_data = np.loadtxt(DATA_DIR / \"Euclid_len.txt\")\n",
    "zl_lens = Euclid_GG_data[:, 0]\n",
    "zs_lens = Euclid_GG_data[:, 1]\n",
    "Ein_lens = Euclid_GG_data[:, 2]\n",
    "re_lens = Euclid_GG_data[:, 5]\n",
    "\n",
    "mask_lens = (Ein_lens >= 0.6) & (re_lens >= 0.25) & (re_lens <= 2.8)\n",
    "zl_lens = zl_lens[mask_lens]\n",
    "zs_lens = zs_lens[mask_lens]\n",
    "thetaE_lens = Ein_lens[mask_lens]\n",
    "re_lens = re_lens[mask_lens]\n",
    "\n",
    "dl_lens, ds_lens, dls_lens = tool.dldsdls(zl_lens, zs_lens, cosmo_true, n=20)\n",
    "N_lens = len(zl_lens)\n",
    "\n",
    "gamma_true_lens = tool.truncated_normal(2.0, 0.2, 1.5, 2.5, N_lens, random_state=rng_np)\n",
    "beta_true_lens  = tool.truncated_normal(0.0, 0.2, -0.4, 0.4, N_lens, random_state=rng_np)\n",
    "vel_model_lens = jampy_interp(thetaE_lens, gamma_true_lens, re_lens, beta_true_lens) * jnp.sqrt(ds_lens / dls_lens)\n",
    "lambda_true_lens = tool.truncated_normal(1.0, 0.05, 0.8, 1.2, N_lens, random_state=rng_np)\n",
    "vel_true_lens = vel_model_lens * jnp.sqrt(lambda_true_lens)\n",
    "\n",
    "theta_E_err = 0.01 * thetaE_lens\n",
    "vel_err_lens = 0.10 * vel_true_lens\n",
    "\n",
    "gamma_obs_clean = gamma_true_lens\n",
    "thetaE_obs_clean = thetaE_lens\n",
    "vel_obs_clean = vel_true_lens\n",
    "\n",
    "gamma_obs_noisy = gamma_true_lens + tool.truncated_normal(0.0, 0.05, -0.2, 0.2, N_lens, random_state=rng_np)\n",
    "thetaE_obs_noisy = thetaE_lens + np.random.normal(0.0, theta_E_err)\n",
    "vel_obs_noisy = np.random.normal(vel_true_lens, vel_err_lens)\n",
    "\n",
    "\n",
    "def build_lens(gamma_obs, theta_E_obs, vel_obs):\n",
    "    return {\n",
    "        \"zl\": zl_lens,\n",
    "        \"zs\": zs_lens,\n",
    "        \"theta_E\": theta_E_obs,\n",
    "        \"theta_E_err\": theta_E_err,\n",
    "        \"re\": re_lens,\n",
    "        \"gamma_obs\": gamma_obs,\n",
    "        \"vel_obs\": vel_obs,\n",
    "        \"vel_err\": vel_err_lens,\n",
    "    }\n",
    "\n",
    "lens_clean = build_lens(gamma_obs_clean, thetaE_obs_clean, vel_obs_clean)\n",
    "lens_noisy = build_lens(gamma_obs_noisy, thetaE_obs_noisy, vel_obs_noisy)\n",
    "\n",
    "# ---------------------------\n",
    "# SNe time-delay data (clean + noisy)\n",
    "# ---------------------------\n",
    "step(\"Build SNe clean/noisy datasets\")\n",
    "sn_data = pd.read_csv(DATA_DIR / \"Euclid_150SNe.csv\")\n",
    "sn_data = sn_data[(sn_data[\"tmax\"] >= 5) & (sn_data[\"tmax\"] <= 80)]\n",
    "sn_data = sn_data.nlargest(100, \"tmax\")\n",
    "zl_sne = np.array(sn_data[\"zl\"])\n",
    "zs_sne = np.array(sn_data[\"z_host\"])\n",
    "t_delay_true_days = np.array(sn_data[\"tmax\"])\n",
    "\n",
    "Dl_sne, Ds_sne, Dls_sne = tool.dldsdls(zl_sne, zs_sne, cosmo_true, n=20)\n",
    "Ddt_geom_sne = (1.0 + zl_sne) * Dl_sne * Ds_sne / Dls_sne\n",
    "\n",
    "N_sne = len(zl_sne)\n",
    "\n",
    "lambda_pop_mean = 1.0\n",
    "lambda_pop_sigma = 0.05\n",
    "lambda_low, lambda_high = 0.8, 1.2\n",
    "lambda_true_sne = tool.truncated_normal(lambda_pop_mean, lambda_pop_sigma, lambda_low, lambda_high, N_sne, random_state=rng_np)\n",
    "\n",
    "seconds_per_day = 86400.0\n",
    "Mpc_km = tool.Mpc / 1000.0\n",
    "\n",
    "fermat_phi_true = (tool.c_km_s * t_delay_true_days * seconds_per_day) / (Ddt_geom_sne * Mpc_km)\n",
    "\n",
    "t_delay_true_mst = t_delay_true_days * lambda_true_sne\n",
    "\n",
    "sigma_t_days = 1.0\n",
    "sigma_phi_frac = 0.04\n",
    "sigma_lambda_frac = 0.08\n",
    "\n",
    "if True:\n",
    "    t_delay_obs_clean = t_delay_true_mst.copy()\n",
    "    fermat_phi_obs_clean = fermat_phi_true.copy()\n",
    "    lambda_obs_clean = lambda_true_sne.copy()\n",
    "\n",
    "    t_delay_obs_noisy = t_delay_true_mst + np.random.normal(0.0, sigma_t_days, size=N_sne)\n",
    "    fermat_phi_obs_noisy = fermat_phi_true + np.random.normal(0.0, sigma_phi_frac * np.abs(fermat_phi_true))\n",
    "    lambda_obs_noisy = lambda_true_sne + np.random.normal(0.0, sigma_lambda_frac * np.abs(lambda_true_sne))\n",
    "\n",
    "lambda_err_sne = sigma_lambda_frac * np.abs(lambda_obs_clean)\n",
    "\n",
    "\n",
    "def scale_phi(phi_obs):\n",
    "    finite = np.isfinite(phi_obs) & (phi_obs != 0)\n",
    "    if not np.any(finite):\n",
    "        return phi_obs, 1.0\n",
    "    median = np.median(np.abs(phi_obs[finite]))\n",
    "    if (not np.isfinite(median)) or median == 0:\n",
    "        return phi_obs, 1.0\n",
    "    exp = int(np.round(-np.log10(median)))\n",
    "    scale = 10.0 ** exp\n",
    "    return phi_obs * scale, scale\n",
    "\n",
    "fermat_phi_obs_clean_scaled, phi_scale_sne = scale_phi(fermat_phi_obs_clean)\n",
    "fermat_phi_obs_noisy_scaled = fermat_phi_obs_noisy * phi_scale_sne\n",
    "\n",
    "sne_clean = {\n",
    "    \"zl\": zl_sne,\n",
    "    \"zs\": zs_sne,\n",
    "    \"t_obs\": t_delay_obs_clean,\n",
    "    \"phi_obs\": fermat_phi_obs_clean_scaled,\n",
    "    \"phi_scale\": phi_scale_sne,\n",
    "    \"lambda_obs\": lambda_obs_clean,\n",
    "    \"lambda_err\": lambda_err_sne,\n",
    "}\n",
    "\n",
    "sne_noisy = {\n",
    "    \"zl\": zl_sne,\n",
    "    \"zs\": zs_sne,\n",
    "    \"t_obs\": t_delay_obs_noisy,\n",
    "    \"phi_obs\": fermat_phi_obs_noisy_scaled,\n",
    "    \"phi_scale\": phi_scale_sne,\n",
    "    \"lambda_obs\": lambda_obs_noisy,\n",
    "    \"lambda_err\": lambda_err_sne,\n",
    "}\n",
    "\n",
    "# ---------------------------\n",
    "# Quasar time-delay data (clean + noisy)\n",
    "# ---------------------------\n",
    "step(\"Build quasar clean/noisy datasets\")\n",
    "DATA_JSON = Path(\"../Temp_data/static_datavectors_seed6.json\")\n",
    "with DATA_JSON.open(\"r\") as f:\n",
    "    quasar_blocks = json.load(f)\n",
    "\n",
    "z_lens_list = []\n",
    "z_src_list = []\n",
    "t_base_list = []\n",
    "t_err_list = []\n",
    "block_id_list = []\n",
    "\n",
    "for b, block in enumerate(quasar_blocks):\n",
    "    z_lens = np.asarray(block[\"z_lens\"], dtype=float)\n",
    "    z_src = np.asarray(block[\"z_src\"], dtype=float)\n",
    "\n",
    "    td = np.asarray(block[\"td_measured\"], dtype=float)\n",
    "    td_mean = td.mean(axis=1)\n",
    "\n",
    "    n_lens, n_td = td_mean.shape\n",
    "    if n_td == 3:\n",
    "        idx = np.argmax(np.abs(td_mean), axis=1)\n",
    "        t_base = np.abs(td_mean[np.arange(n_lens), idx])\n",
    "    else:\n",
    "        t_base = np.abs(td_mean[:, 0])\n",
    "\n",
    "    if b in (0, 1, 2):\n",
    "        t_err = 0.03 * t_base\n",
    "    else:\n",
    "        t_err = np.full_like(t_base, 5.0)\n",
    "\n",
    "    z_lens_list.append(z_lens)\n",
    "    z_src_list.append(z_src)\n",
    "    t_base_list.append(t_base)\n",
    "    t_err_list.append(t_err)\n",
    "    block_id_list.append(np.full(n_lens, b, dtype=int))\n",
    "\n",
    "z_lens_q = np.concatenate(z_lens_list)\n",
    "z_src_q = np.concatenate(z_src_list)\n",
    "t_base_q = np.concatenate(t_base_list)\n",
    "t_err_q = np.concatenate(t_err_list)\n",
    "block_id_q = np.concatenate(block_id_list)\n",
    "\n",
    "zl_j = jnp.asarray(z_lens_q)\n",
    "zs_j = jnp.asarray(z_src_q)\n",
    "Dl_q, Ds_q, Dls_q = tool.dldsdls(zl_j, zs_j, cosmo_true, n=20)\n",
    "Ddt_geom_q = (1.0 + zl_j) * Dl_q * Ds_q / Dls_q\n",
    "Ddt_geom_q = np.asarray(Ddt_geom_q)\n",
    "\n",
    "c_km_day = tool.c_km_s * 86400.0\n",
    "Mpc_km = tool.Mpc / 1000.0\n",
    "\n",
    "phi_true_q = (c_km_day * t_base_q) / (Ddt_geom_q * Mpc_km)\n",
    "\n",
    "phi_err_frac_by_block = {\n",
    "    0: 0.02,\n",
    "    1: 0.05,\n",
    "    2: 0.05,\n",
    "    3: 0.11,\n",
    "    4: 0.11,\n",
    "    5: 0.18,\n",
    "    6: 0.18,\n",
    "    7: 0.18,\n",
    "    8: 0.18,\n",
    "}\n",
    "\n",
    "sigma_v_frac_by_block = {\n",
    "    0: 0.03,\n",
    "    1: 0.03,\n",
    "    2: 0.03,\n",
    "    3: 0.10,\n",
    "    4: 0.10,\n",
    "    5: 0.10,\n",
    "    6: 0.10,\n",
    "    7: np.nan,\n",
    "    8: np.nan,\n",
    "}\n",
    "\n",
    "phi_err_frac_q = np.asarray([phi_err_frac_by_block[b] for b in block_id_q])\n",
    "phi_err_q = phi_err_frac_q * np.abs(phi_true_q)\n",
    "\n",
    "lambda_true_q = tool.truncated_normal(1.0, 0.05, 0.8, 1.2, z_lens_q.size, random_state=rng_np)\n",
    "\n",
    "sigma_v_frac_q = np.asarray([sigma_v_frac_by_block[b] for b in block_id_q])\n",
    "mst_mask_q = np.isfinite(sigma_v_frac_q)\n",
    "mst_err_frac_q = 2.0 * sigma_v_frac_q\n",
    "lambda_err_q = np.where(mst_mask_q, mst_err_frac_q * np.abs(lambda_true_q), 0.05)\n",
    "\n",
    "t_true_q = t_base_q * lambda_true_q\n",
    "\n",
    "phi_obs_q_clean = phi_true_q.copy()\n",
    "phi_obs_q_noisy = phi_true_q + rng_np.normal(0.0, phi_err_q)\n",
    "\n",
    "phi_obs_q_clean_scaled, phi_scale_q = scale_phi(phi_obs_q_clean)\n",
    "phi_obs_q_noisy_scaled = phi_obs_q_noisy * phi_scale_q\n",
    "\n",
    "lambda_obs_q_clean = lambda_true_q.copy()\n",
    "lambda_obs_q_noisy = lambda_true_q + rng_np.normal(0.0, lambda_err_q)\n",
    "\n",
    "t_obs_q_clean = t_true_q.copy()\n",
    "t_obs_q_noisy = t_true_q + rng_np.normal(0.0, t_err_q)\n",
    "\n",
    "quasar_clean = {\n",
    "    \"zl\": z_lens_q,\n",
    "    \"zs\": z_src_q,\n",
    "    \"t_obs\": t_obs_q_clean,\n",
    "    \"t_err\": t_err_q,\n",
    "    \"phi_obs\": phi_obs_q_clean_scaled,\n",
    "    \"phi_err\": phi_err_frac_q * np.abs(phi_obs_q_clean_scaled),\n",
    "    \"phi_scale\": phi_scale_q,\n",
    "    \"lambda_obs\": lambda_obs_q_clean,\n",
    "    \"lambda_err\": lambda_err_q,\n",
    "    \"mst_mask\": mst_mask_q,\n",
    "}\n",
    "\n",
    "quasar_noisy = {\n",
    "    \"zl\": z_lens_q,\n",
    "    \"zs\": z_src_q,\n",
    "    \"t_obs\": t_obs_q_noisy,\n",
    "    \"t_err\": t_err_q,\n",
    "    \"phi_obs\": phi_obs_q_noisy_scaled,\n",
    "    \"phi_err\": phi_err_frac_q * np.abs(phi_obs_q_noisy_scaled),\n",
    "    \"phi_scale\": phi_scale_q,\n",
    "    \"lambda_obs\": lambda_obs_q_noisy,\n",
    "    \"lambda_err\": lambda_err_q,\n",
    "    \"mst_mask\": mst_mask_q,\n",
    "}\n",
    "\n",
    "\n",
    "def head_dict(data_dict, N_use=None):\n",
    "    out = {}\n",
    "    for k, v in data_dict.items():\n",
    "        arr = np.asarray(v)\n",
    "        if arr.shape == ():\n",
    "            out[k] = arr\n",
    "        else:\n",
    "            out[k] = arr[:N_use] if N_use is not None else arr\n",
    "    return out\n",
    "\n",
    "if TEST_MODE:\n",
    "    N_DSPL_USE = 50\n",
    "    N_LENS_USE = 200\n",
    "    N_SNE_USE = 10\n",
    "    N_QUASAR_USE = 30\n",
    "    num_warmup = 200\n",
    "    num_samples = 200\n",
    "    num_chains = 2\n",
    "    chain_method = \"sequential\"\n",
    "else:\n",
    "    N_DSPL_USE = None\n",
    "    N_LENS_USE = None\n",
    "    N_SNE_USE = None\n",
    "    N_QUASAR_USE = None\n",
    "    num_warmup = 500\n",
    "    num_samples = 1500\n",
    "    num_chains = 4\n",
    "    chain_method = \"vectorized\"\n",
    "\n",
    "step(\"Apply per-probe sample limits for this run mode\")\n",
    "\n",
    "dspl_clean = head_dict(dspl_clean, N_DSPL_USE)\n",
    "Lens_clean = head_dict(lens_clean, N_LENS_USE)\n",
    "sne_clean = head_dict(sne_clean, N_SNE_USE)\n",
    "quasar_clean = head_dict(quasar_clean, N_QUASAR_USE)\n",
    "\n",
    "dspl_noisy = head_dict(dspl_noisy, N_DSPL_USE)\n",
    "Lens_noisy = head_dict(lens_noisy, N_LENS_USE)\n",
    "sne_noisy = head_dict(sne_noisy, N_SNE_USE)\n",
    "quasar_noisy = head_dict(quasar_noisy, N_QUASAR_USE)\n",
    "\n",
    "\n",
    "def cosmology_model(kind, cosmo_prior, sample_h0=True):\n",
    "    cosmo = {\n",
    "        \"Omegam\": numpyro.sample(\"Omegam\", dist.Uniform(cosmo_prior[\"omegam_low\"], cosmo_prior[\"omegam_up\"])),\n",
    "        \"Omegak\": 0.0,\n",
    "        \"w0\": -1.0,\n",
    "        \"wa\": 0.0,\n",
    "        \"h0\": 70.0,\n",
    "    }\n",
    "    if kind in [\"wcdm\", \"owcdm\", \"waw0cdm\", \"owaw0cdm\"]:\n",
    "        cosmo[\"w0\"] = numpyro.sample(\"w0\", dist.Uniform(cosmo_prior[\"w0_low\"], cosmo_prior[\"w0_up\"]))\n",
    "    if kind in [\"waw0cdm\", \"owaw0cdm\"]:\n",
    "        cosmo[\"wa\"] = numpyro.sample(\"wa\", dist.Uniform(cosmo_prior[\"wa_low\"], cosmo_prior[\"wa_up\"]))\n",
    "    if kind in [\"owcdm\", \"owaw0cdm\"]:\n",
    "        cosmo[\"Omegak\"] = numpyro.sample(\"Omegak\", dist.Uniform(cosmo_prior[\"omegak_low\"], cosmo_prior[\"omegak_up\"]))\n",
    "    if sample_h0:\n",
    "        cosmo[\"h0\"] = numpyro.sample(\"h0\", dist.Uniform(cosmo_prior[\"h0_low\"], cosmo_prior[\"h0_up\"]))\n",
    "    return cosmo\n",
    "\n",
    "\n",
    "def joint_model(dspl_data=None, lens_data=None, sne_data=None, quasar_data=None):\n",
    "    cosmo = cosmology_model(\"waw0cdm\", cosmo_prior, sample_h0=True)\n",
    "\n",
    "    lambda_mean = numpyro.sample(\"lambda_mean\", dist.Uniform(0.9, 1.1))\n",
    "    lambda_sigma = numpyro.sample(\"lambda_sigma\", dist.TruncatedNormal(0.05, 0.5, low=0.0, high=0.2))\n",
    "\n",
    "    gamma_mean = numpyro.sample(\"gamma_mean\", dist.Uniform(1.4, 2.6))\n",
    "    gamma_sigma = numpyro.sample(\"gamma_sigma\", dist.TruncatedNormal(0.2, 0.2, low=0.0, high=0.4))\n",
    "\n",
    "    beta_mean = numpyro.sample(\"beta_mean\", dist.Uniform(-0.3, 0.3))\n",
    "    beta_sigma = numpyro.sample(\"beta_sigma\", dist.TruncatedNormal(0.2, 0.2, low=0.0, high=0.4))\n",
    "\n",
    "    if dspl_data is not None:\n",
    "        zl = jnp.asarray(dspl_data[\"zl\"])\n",
    "        zs1 = jnp.asarray(dspl_data[\"zs1\"])\n",
    "        zs2_obs = jnp.asarray(dspl_data[\"zs2_obs\"])\n",
    "        zs2_err = jnp.asarray(dspl_data[\"zs2_err\"])\n",
    "\n",
    "        Dl1, Ds1, Dls1 = tool.compute_distances(zl, zs1, cosmo)\n",
    "\n",
    "        eps = 1e-3\n",
    "        zs2_true = numpyro.sample(\n",
    "            \"zs2_true\",\n",
    "            dist.TruncatedNormal(zs2_obs, zs2_err, low=zs1 + eps, high=10.0).to_event(1),\n",
    "        )\n",
    "\n",
    "        Dl2, Ds2, Dls2 = tool.compute_distances(zl, zs2_true, cosmo)\n",
    "        beta_geom = Dls1 * Ds2 / (Ds1 * Dls2)\n",
    "\n",
    "        N = len(zl)\n",
    "        with numpyro.plate(\"dspl\", N):\n",
    "            lambda_dspl = numpyro.sample(\"lambda_dspl\", dist.TruncatedNormal(lambda_mean, lambda_sigma, low=0.8, high=1.2))\n",
    "            numpyro.sample(\"lambda_dspl_like\", dist.Normal(lambda_dspl, dspl_data[\"lambda_err\"]), obs=dspl_data[\"lambda_obs\"])\n",
    "            beta_mst = tool.beta_antimst(beta_geom, lambda_dspl)\n",
    "            numpyro.sample(\"beta_dspl_like\", dist.TruncatedNormal(beta_mst, dspl_data[\"beta_err\"], low=0.0, high=1.0), obs=dspl_data[\"beta_obs\"])\n",
    "\n",
    "    if lens_data is not None:\n",
    "        dl_lens, ds_lens, dls_lens = tool.dldsdls(lens_data[\"zl\"], lens_data[\"zs\"], cosmo, n=20)\n",
    "        N_lens = len(lens_data[\"zl\"])\n",
    "        with numpyro.plate(\"lens\", N_lens):\n",
    "            gamma_i = numpyro.sample(\"gamma_i\", dist.TruncatedNormal(gamma_mean, gamma_sigma, low=1.4, high=2.6))\n",
    "            beta_i = numpyro.sample(\"beta_i\", dist.TruncatedNormal(beta_mean, beta_sigma, low=-0.4, high=0.4))\n",
    "            lambda_lens = numpyro.sample(\"lambda_lens\", dist.TruncatedNormal(lambda_mean, lambda_sigma, low=0.8, high=1.2))\n",
    "            theta_E_i = numpyro.sample(\"theta_E_i\", dist.Normal(lens_data[\"theta_E\"], lens_data[\"theta_E_err\"]))\n",
    "            v_interp = jampy_interp(theta_E_i, gamma_i, lens_data[\"re\"], beta_i)\n",
    "            vel_pred = v_interp * jnp.sqrt(ds_lens / dls_lens) * jnp.sqrt(lambda_lens)\n",
    "\n",
    "            numpyro.sample(\"gamma_obs_lens\", dist.Normal(gamma_i, 0.05), obs=lens_data[\"gamma_obs\"])\n",
    "            numpyro.sample(\"vel_lens_like\", dist.Normal(vel_pred, lens_data[\"vel_err\"]), obs=lens_data[\"vel_obs\"])\n",
    "\n",
    "    if sne_data is not None:\n",
    "        Dl_sne, Ds_sne, Dls_sne = tool.dldsdls(sne_data[\"zl\"], sne_data[\"zs\"], cosmo, n=20)\n",
    "        Ddt_geom = (1.0 + sne_data[\"zl\"]) * Dl_sne * Ds_sne / Dls_sne\n",
    "        N_sne = len(sne_data[\"zl\"])\n",
    "\n",
    "        t_obs = sne_data[\"t_obs\"]\n",
    "        phi_obs = sne_data[\"phi_obs\"]\n",
    "        phi_scale = sne_data[\"phi_scale\"]\n",
    "        lambda_obs = sne_data[\"lambda_obs\"]\n",
    "        lambda_err = sne_data[\"lambda_err\"]\n",
    "        sigma_phi = sigma_phi_frac * phi_obs\n",
    "\n",
    "        with numpyro.plate(\"sne\", N_sne):\n",
    "            phi_true_scaled = numpyro.sample(\"phi_true_scaled_sne\", dist.TruncatedNormal(phi_obs, sigma_phi, low=0.0, high=10.0))\n",
    "            lambda_sne = numpyro.sample(\"lambda_sne\", dist.TruncatedNormal(lambda_mean, lambda_sigma, low=0.8, high=1.2))\n",
    "            numpyro.sample(\"lambda_sne_like\", dist.Normal(lambda_sne, lambda_err), obs=lambda_obs)\n",
    "\n",
    "            phi_true = phi_true_scaled / phi_scale\n",
    "            Ddt_true = Ddt_geom * lambda_sne\n",
    "            t_model_days = (Ddt_true * Mpc_km / tool.c_km_s) * phi_true / seconds_per_day\n",
    "            numpyro.sample(\"t_delay_sne_like\", dist.Normal(t_model_days, sigma_t_days), obs=t_obs)\n",
    "\n",
    "    if quasar_data is not None:\n",
    "        Dl_q, Ds_q, Dls_q = tool.dldsdls(quasar_data[\"zl\"], quasar_data[\"zs\"], cosmo, n=20)\n",
    "        Ddt_geom_q = (1.0 + quasar_data[\"zl\"]) * Dl_q * Ds_q / Dls_q\n",
    "        N_q = len(quasar_data[\"zl\"])\n",
    "\n",
    "        t_obs = quasar_data[\"t_obs\"]\n",
    "        t_err = quasar_data[\"t_err\"]\n",
    "        phi_obs = quasar_data[\"phi_obs\"]\n",
    "        phi_err = quasar_data[\"phi_err\"]\n",
    "        phi_scale = quasar_data[\"phi_scale\"]\n",
    "        lambda_obs = quasar_data[\"lambda_obs\"]\n",
    "        lambda_err = quasar_data[\"lambda_err\"]\n",
    "        mst_mask = jnp.asarray(quasar_data[\"mst_mask\"])\n",
    "\n",
    "        with numpyro.plate(\"quasar\", N_q):\n",
    "            phi_true_scaled = numpyro.sample(\"phi_true_scaled_q\", dist.Normal(phi_obs, phi_err))\n",
    "            lambda_q = numpyro.sample(\"lambda_q\", dist.TruncatedNormal(lambda_mean, lambda_sigma, low=0.8, high=1.2))\n",
    "            numpyro.sample(\"lambda_q_like\", dist.Normal(lambda_q, lambda_err).mask(mst_mask), obs=lambda_obs)\n",
    "\n",
    "            phi_true = phi_true_scaled / phi_scale\n",
    "            Ddt_true = Ddt_geom_q * lambda_q\n",
    "            t_model_days = (Ddt_true * Mpc_km / tool.c_km_s) * phi_true / seconds_per_day\n",
    "            numpyro.sample(\"t_delay_q_like\", dist.Normal(t_model_days, t_err), obs=t_obs)\n",
    "\n",
    "\n",
    "def build_init_values(data):\n",
    "    init_values = {\n",
    "        \"Omegam\": jnp.asarray(cosmo_true[\"Omegam\"]),\n",
    "        \"w0\": jnp.asarray(cosmo_true[\"w0\"]),\n",
    "        \"wa\": jnp.asarray(cosmo_true[\"wa\"]),\n",
    "        \"h0\": jnp.asarray(cosmo_true[\"h0\"]),\n",
    "        \"lambda_mean\": jnp.asarray(1.0),\n",
    "        \"lambda_sigma\": jnp.asarray(0.08),\n",
    "        \"gamma_mean\": jnp.asarray(2.0),\n",
    "        \"gamma_sigma\": jnp.asarray(0.25),\n",
    "        \"beta_mean\": jnp.asarray(0.0),\n",
    "        \"beta_sigma\": jnp.asarray(0.25),\n",
    "    }\n",
    "\n",
    "    dspl_data = data.get(\"dspl\")\n",
    "    if dspl_data is not None:\n",
    "        zs2_true = np.asarray(dspl_data[\"zs2_obs\"], dtype=np.float64)\n",
    "        zs1 = np.asarray(dspl_data[\"zs1\"], dtype=np.float64)\n",
    "        zs2_true = np.maximum(zs2_true, zs1 + 1e-3)\n",
    "        zs2_true = np.clip(zs2_true, zs1 + 1e-3, 9.999)\n",
    "        lambda_dspl = np.asarray(dspl_data[\"lambda_obs\"], dtype=np.float64)\n",
    "        lambda_dspl = np.clip(lambda_dspl, 0.801, 1.199)\n",
    "        init_values[\"zs2_true\"] = jnp.asarray(zs2_true)\n",
    "        init_values[\"lambda_dspl\"] = jnp.asarray(lambda_dspl)\n",
    "\n",
    "    lens_data = data.get(\"lens\")\n",
    "    if lens_data is not None:\n",
    "        gamma_i = np.asarray(lens_data[\"gamma_obs\"], dtype=np.float64)\n",
    "        gamma_i = np.clip(gamma_i, 1.401, 2.599)\n",
    "        beta_i = np.zeros_like(gamma_i, dtype=np.float64)\n",
    "        lambda_lens = np.ones_like(gamma_i, dtype=np.float64)\n",
    "        theta_E_i = np.asarray(lens_data[\"theta_E\"], dtype=np.float64)\n",
    "        theta_E_i = np.maximum(theta_E_i, 1e-3)\n",
    "        init_values[\"gamma_i\"] = jnp.asarray(gamma_i)\n",
    "        init_values[\"beta_i\"] = jnp.asarray(beta_i)\n",
    "        init_values[\"lambda_lens\"] = jnp.asarray(lambda_lens)\n",
    "        init_values[\"theta_E_i\"] = jnp.asarray(theta_E_i)\n",
    "\n",
    "    sne_data = data.get(\"sne\")\n",
    "    if sne_data is not None:\n",
    "        phi_true_scaled_sne = np.asarray(sne_data[\"phi_obs\"], dtype=np.float64)\n",
    "        phi_true_scaled_sne = np.clip(phi_true_scaled_sne, 1e-3, 9.999)\n",
    "        lambda_sne = np.asarray(sne_data[\"lambda_obs\"], dtype=np.float64)\n",
    "        lambda_sne = np.clip(lambda_sne, 0.801, 1.199)\n",
    "        init_values[\"phi_true_scaled_sne\"] = jnp.asarray(phi_true_scaled_sne)\n",
    "        init_values[\"lambda_sne\"] = jnp.asarray(lambda_sne)\n",
    "\n",
    "    quasar_data = data.get(\"quasar\")\n",
    "    if quasar_data is not None:\n",
    "        phi_true_scaled_q = np.asarray(quasar_data[\"phi_obs\"], dtype=np.float64)\n",
    "        lambda_q = np.asarray(quasar_data[\"lambda_obs\"], dtype=np.float64)\n",
    "        lambda_q = np.clip(lambda_q, 0.801, 1.199)\n",
    "        init_values[\"phi_true_scaled_q\"] = jnp.asarray(phi_true_scaled_q)\n",
    "        init_values[\"lambda_q\"] = jnp.asarray(lambda_q)\n",
    "\n",
    "    return init_values\n",
    "\n",
    "\n",
    "def run_mcmc(data, key, tag):\n",
    "    step(f\"Run joint MCMC ({tag})\")\n",
    "    nuts = NUTS(\n",
    "        joint_model,\n",
    "        target_accept_prob=0.99,\n",
    "        init_strategy=init_to_value(values=build_init_values(data)),\n",
    "    )\n",
    "    mcmc = MCMC(\n",
    "        nuts,\n",
    "        num_warmup=num_warmup,\n",
    "        num_samples=num_samples,\n",
    "        num_chains=num_chains,\n",
    "        chain_method=chain_method,\n",
    "        progress_bar=True,\n",
    "    )\n",
    "    mcmc.run(\n",
    "        key,\n",
    "        dspl_data=data[\"dspl\"],\n",
    "        lens_data=data[\"lens\"],\n",
    "        sne_data=data[\"sne\"],\n",
    "        quasar_data=data[\"quasar\"],\n",
    "    )\n",
    "    extra = mcmc.get_extra_fields(group_by_chain=True)\n",
    "    n_div = int(np.asarray(extra[\"diverging\"]).sum())\n",
    "    print(f\"[{tag}] divergences: {n_div}\")\n",
    "    posterior = mcmc.get_samples(group_by_chain=True)\n",
    "    inf_data = az.from_dict(posterior=posterior)\n",
    "    az.to_netcdf(inf_data, RESULT_DIR / f\"joint_{tag}.nc\")\n",
    "    trace_vars = [\"h0\", \"Omegam\", \"w0\", \"wa\", \"lambda_mean\", \"lambda_sigma\", \"gamma_mean\", \"gamma_sigma\", \"beta_mean\", \"beta_sigma\"]\n",
    "    trace_vars = [v for v in trace_vars if v in inf_data.posterior and inf_data.posterior[v].ndim == 2]\n",
    "\n",
    "    valid_trace_vars = []\n",
    "    dropped_trace_vars = {}\n",
    "    for v in trace_vars:\n",
    "        vals = np.asarray(inf_data.posterior[v]).reshape(-1)\n",
    "        finite = np.isfinite(vals)\n",
    "        n_finite = int(finite.sum())\n",
    "        if n_finite < 5:\n",
    "            dropped_trace_vars[v] = f\"finite samples too few ({n_finite})\"\n",
    "            continue\n",
    "        std = float(np.nanstd(vals[finite]))\n",
    "        if (not np.isfinite(std)) or (std < 1e-12):\n",
    "            dropped_trace_vars[v] = f\"near-constant/non-finite std ({std})\"\n",
    "            continue\n",
    "        valid_trace_vars.append(v)\n",
    "\n",
    "    if dropped_trace_vars:\n",
    "        print(f\"[{tag}] skip trace vars with unstable density input:\")\n",
    "        for k, reason in dropped_trace_vars.items():\n",
    "            print(f\"  - {k}: {reason}\")\n",
    "\n",
    "    if valid_trace_vars:\n",
    "        trace_axes = az.plot_trace(inf_data, var_names=valid_trace_vars, compact=False)\n",
    "        trace_fig = np.asarray(trace_axes).ravel()[0].figure\n",
    "        trace_fig.savefig(FIG_DIR / f\"joint_trace_{tag}.pdf\", dpi=200, bbox_inches=\"tight\")\n",
    "        plt.close(trace_fig)\n",
    "    else:\n",
    "        print(f\"[{tag}] no valid vars left for trace plot.\")\n",
    "    return inf_data\n",
    "\n",
    "\n",
    "clean_data = {\"dspl\": dspl_clean, \"lens\": Lens_clean, \"sne\": sne_clean, \"quasar\": quasar_clean}\n",
    "noisy_data = {\"dspl\": dspl_noisy, \"lens\": Lens_noisy, \"sne\": sne_noisy, \"quasar\": quasar_noisy}\n",
    "\n",
    "key = random.PRNGKey(42)\n",
    "key_clean, key_noisy = random.split(key)\n",
    "\n",
    "step(\"Execute clean joint run\")\n",
    "idata_clean = run_mcmc(clean_data, key_clean, \"clean\")\n",
    "\n",
    "if RUN_NOISY_INFERENCE:\n",
    "    step(\"Execute noisy joint run\")\n",
    "    idata_noisy = run_mcmc(noisy_data, key_noisy, \"noisy\")\n",
    "\n",
    "    step(\"Create overlay corner plot\")\n",
    "    corner_vars = select_corner_vars(\n",
    "        idata_clean,\n",
    "        idata_noisy,\n",
    "        [\"h0\", \"Omegam\", \"w0\", \"wa\", \"lambda_mean\", \"lambda_sigma\", \"gamma_mean\", \"gamma_sigma\", \"beta_mean\", \"beta_sigma\"],\n",
    "    )\n",
    "    make_overlay_corner(idata_clean, idata_noisy, corner_vars, FIG_DIR / \"joint_corner_overlay.pdf\")\n",
    "else:\n",
    "    step(\"Skip noisy joint inference (RUN_NOISY_INFERENCE=False)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arviz as az\n",
    "import numpy as np\n",
    "import corner\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "summary_vars = [\n",
    "    v for v in ['h0', 'Omegam', 'w0', 'wa', 'lambda_mean', 'lambda_sigma', 'gamma_mean', 'gamma_sigma', 'beta_mean', 'beta_sigma']\n",
    "    if v in idata_clean.posterior\n",
    "]\n",
    "print(az.summary(idata_clean, var_names=summary_vars))\n",
    "\n",
    "plot_vars = summary_vars\n",
    "samples = np.column_stack([\n",
    "    np.asarray(idata_clean.posterior[v]).reshape(-1)\n",
    "    for v in plot_vars\n",
    "])\n",
    "finite_mask = np.isfinite(samples).all(axis=1)\n",
    "samples = samples[finite_mask]\n",
    "\n",
    "fig = corner.corner(\n",
    "    samples,\n",
    "    labels=plot_vars,\n",
    "    show_titles=True,\n",
    "    title_fmt='.3f',\n",
    "    quantiles=[0.16, 0.5, 0.84],\n",
    ")\n",
    "\n",
    "pdf_out = FIG_DIR / 'joint_clean_corner_from_test_notebook.pdf'\n",
    "png_out = FIG_DIR / 'joint_clean_corner_from_test_notebook.png'\n",
    "fig.savefig(pdf_out, dpi=200, bbox_inches='tight')\n",
    "fig.savefig(png_out, dpi=200, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print('Saved:', pdf_out)\n",
    "print('Saved:', png_out)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}